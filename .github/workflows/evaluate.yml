name: Model Evaluation

on:
  workflow_run:
    workflows: ["Run Tests and Lint"]
    types:
      - completed

jobs:
  evaluate:
    runs-on: ubuntu-latest
    if: ${{ github.event.workflow_run.conclusion == 'success' }}

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Set Up Python Environment
        uses: actions/setup-python@v4
        with:
          python-version: "3.10.11"

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run Model Evaluation
        run: |
          python tests/evaluation/evaluate_model.py 

      - name: Upload Evaluation Artifacts
        uses: actions/upload-artifact@v3
        with:
          name: evaluation-artifacts
          path: output/model_output/evaluation/test_run_*/

      - name: Check Performance Thresholds
        run: |
          # Get the latest test run folder
          LATEST_TEST_RUN=$(ls -td output/model_output/evaluation/test_run_* | head -n 1)

          if [ -z "$LATEST_TEST_RUN" ]; then
            echo "❌ No evaluation results found!"
            exit 1
          fi

          for config in "merged_labels 0.60" "original_labels 0.50"; do
            MODEL_NAME=$(echo $config | awk '{print $1}')
            THRESHOLD=$(echo $config | awk '{print $2}')
            
            METRICS_FILE="$LATEST_TEST_RUN/$MODEL_NAME/metrics.txt"
            
            if [ -f "$METRICS_FILE" ]; then
              ACCURACY=$(grep "Validation Accuracy" "$METRICS_FILE" | sed 's/[^0-9.]//g')
              echo "$MODEL_NAME Model Accuracy: $ACCURACY (Threshold: $THRESHOLD)"

              if [ $(echo "$ACCURACY < $THRESHOLD" | bc -l) -eq 1 ]; then
                echo "❌ $MODEL_NAME model accuracy is below threshold ($THRESHOLD). Failing the workflow."
                exit 1
              fi
            else
              echo "⚠️ Metrics file not found for $MODEL_NAME model."
              exit 1
            fi
          done
